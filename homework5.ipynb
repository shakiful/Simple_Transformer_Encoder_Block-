{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ==========================================\n",
        "# Q1. Compute Scaled Dot-Product Attention (Python/NumPy)\n",
        "# ==========================================\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Computes the scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        Q (np.array): Query matrix of shape (seq_len, d_k)\n",
        "        K (np.array): Key matrix of shape (seq_len, d_k)\n",
        "        V (np.array): Value matrix of shape (seq_len, d_v)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (context_vector, attention_weights)\n",
        "    \"\"\"\n",
        "    # 1. Determine dimension d_k for scaling\n",
        "    d_k = Q.shape[-1]\n",
        "\n",
        "    # 2. Compute scores: (Q . K^T) / sqrt(d_k)\n",
        "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
        "\n",
        "    # 3. Normalize scores using softmax\n",
        "    # We subtract max for numerical stability before exp\n",
        "    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "\n",
        "    # 4. Compute context vector: weights . V\n",
        "    context_vector = np.dot(attention_weights, V)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "# --- Testing Q1 ---\n",
        "print(\"--- Q1: Scaled Dot-Product Attention Test ---\")\n",
        "np.random.seed(42)\n",
        "seq_len = 3\n",
        "d_model = 4\n",
        "Q_np = np.random.rand(seq_len, d_model)\n",
        "K_np = np.random.rand(seq_len, d_model)\n",
        "V_np = np.random.rand(seq_len, d_model)\n",
        "\n",
        "context, weights = scaled_dot_product_attention(Q_np, K_np, V_np)\n",
        "print(\"Attention Weights shape:\", weights.shape)\n",
        "print(\"Context Vector shape:\", context.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Q2. Implement Simple Transformer Encoder Block (PyTorch)\n",
        "# ==========================================\n",
        "\n",
        "class SimpleTransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_head, ff_hidden=256, dropout=0.1):\n",
        "        super(SimpleTransformerEncoderBlock, self).__init__()\n",
        "\n",
        "        # a) Initialize dimensions dmodel=128, h=8 (passed in arguments)\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_head, batch_first=True)\n",
        "\n",
        "        # b) Add residual connections and layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Feed-forward network (2 linear layers with ReLU)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden, d_model)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 1. Multi-Head Attention Sub-layer\n",
        "        # The attention layer returns (attn_output, attn_weights)\n",
        "        attn_output, _ = self.self_attn(x, x, x) # Self-attention: Q=K=V=x\n",
        "\n",
        "        # Add & Norm (Residual connection + LayerNorm)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # 2. Feed-Forward Sub-layer\n",
        "        ff_output = self.feed_forward(x)\n",
        "\n",
        "        # Add & Norm (Residual connection + LayerNorm)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "# --- Testing Q2 ---\n",
        "print(\"--- Q2: Transformer Encoder Block Test ---\")\n",
        "\n",
        "# c) Verify the output shape for a batch of 32 sentences, each with 10 tokens\n",
        "BATCH_SIZE = 32\n",
        "SEQ_LEN = 10\n",
        "D_MODEL = 128\n",
        "N_HEAD = 8\n",
        "\n",
        "# Instantiate model\n",
        "encoder_block = SimpleTransformerEncoderBlock(d_model=D_MODEL, n_head=N_HEAD)\n",
        "\n",
        "# Create dummy input (Batch, Seq_Len, D_Model)\n",
        "dummy_input = torch.randn(BATCH_SIZE, SEQ_LEN, D_MODEL)\n",
        "\n",
        "# Forward pass\n",
        "output = encoder_block(dummy_input)\n",
        "\n",
        "print(f\"Input shape: {dummy_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "# Verification\n",
        "if output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL):\n",
        "    print(\"SUCCESS: Output shape matches requirements.\")\n",
        "else:\n",
        "    print(\"FAILURE: Output shape incorrect.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXseBFTLsvMs",
        "outputId": "f31d95a6-0be4-4c25-beba-3bef80e31176"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Q1: Scaled Dot-Product Attention Test ---\n",
            "Attention Weights shape: (3, 3)\n",
            "Context Vector shape: (3, 4)\n",
            "\n",
            "\n",
            "--- Q2: Transformer Encoder Block Test ---\n",
            "Input shape: torch.Size([32, 10, 128])\n",
            "Output shape: torch.Size([32, 10, 128])\n",
            "SUCCESS: Output shape matches requirements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "08bajbbompNa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}